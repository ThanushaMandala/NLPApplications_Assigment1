title,authors,journal,year,cited_papers
"Attention Is All You Need","Ashish Vaswani, Noam Shazeer, Niki Parmar","Advances in Neural Information Processing Systems",2017,"Neural Machine Translation by Jointly Learning to Align and Translate, Sequence to Sequence Learning with Neural Networks"
"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","Jacob Devlin, Ming-Wei Chang, Kenton Lee","North American Chapter of the Association for Computational Linguistics",2019,"Attention Is All You Need, Deep contextualized word representations"
"Deep contextualized word representations","Matthew E. Peters, Mark Neumann, Mohit Iyyer","North American Chapter of the Association for Computational Linguistics",2018,"Attention Is All You Need, Neural Machine Translation by Jointly Learning to Align and Translate"
"Neural Machine Translation by Jointly Learning to Align and Translate","Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio","International Conference on Learning Representations",2015,"Sequence to Sequence Learning with Neural Networks"
"Sequence to Sequence Learning with Neural Networks","Ilya Sutskever, Oriol Vinyals, Quoc V. Le","Advances in Neural Information Processing Systems",2014,""
"GPT-3: Language Models are Few-Shot Learners","Tom B. Brown, Benjamin Mann, Nick Ryder","Advances in Neural Information Processing Systems",2020,"Attention Is All You Need, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
"Transformers: State-of-the-Art Natural Language Processing","Thomas Wolf, Lysandre Debut, Victor Sanh","Conference on Empirical Methods in Natural Language Processing",2020,"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Attention Is All You Need"
"RoBERTa: A Robustly Optimized BERT Pretraining Approach","Yinhan Liu, Myle Ott, Naman Goyal","arXiv preprint",2019,"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
"T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer","Colin Raffel, Noam Shazeer, Adam Roberts","Journal of Machine Learning Research",2020,"Attention Is All You Need, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
"XLNet: Generalized Autoregressive Pretraining for Language Understanding","Zhilin Yang, Zihang Dai, Yiming Yang","Advances in Neural Information Processing Systems",2019,"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Attention Is All You Need" 